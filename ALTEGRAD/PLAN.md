# LAB1 : Hierarchical Attention Network Using GRU
Get familiar with recurrent neural networks (RNNs), self-attention, and the HAN architecture. 

# LAB2 : Transfer Learning in NLP
- Implement and pretrain a language model with transformer architecture.
- Use the pretrained model (transfer learning) to perform a sentiment analysis task which consists of classifying some books reviews into positive and negative ones.
- Compare the performance of the pretrained model to a model trained from scratch.

# LAB3 : Introduction to LLMs

# LAB4 : Machine Learning for Graphs
Implement some basic techniques for dealing with different graph mining problems

# LAB5 : Deep Learning for Graphs (1/2)
learn about neural networks that operate on graphs

# LAB6 : Deep Learning for Graphs (2/2)

# LAB7 : Learning on Sets and Graph Generative Models
Introduction to machine learning models for data represented as sets.  Specifically, in the first part of the lab, we will implement the DeepSets model. We will evaluate the model in the task of computing the sum of mul- tisets of digits and compare it against traditional models such as LSTMs. In the second part of the lab, we will learn about the graph generation problem, and you will implement a variational graph autoen- coder to generate stochastic block model graphs.
